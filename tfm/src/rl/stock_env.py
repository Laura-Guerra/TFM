import json
from datetime import date
from pathlib import Path
import gymnasium
import numpy as np
import pandas as pd
from gymnasium import spaces
from loguru import logger
from itertools import product
from tfm.src.config.settings import PATH_DATA_RESULTS


class StockEnvironment(gymnasium.Env):
    metadata = {"render.modes": ["human"]}

    def __init__(self, df, initial_balance=50000.0, continuous_actions=False,
                 model_name="dqn", do_save_history=False, episode_length=200, is_train=False, is_eval=False):
        super().__init__()
        self.df = df.reset_index(drop=True)
        self.assets = sorted(self.df["ticker"].unique())
        self.n_assets = len(self.assets)
        self.initial_balance = initial_balance
        self.continuous_actions = continuous_actions
        self.episode_length = episode_length
        self.model_name = model_name
        self.do_save_history = do_save_history
        self.is_train = is_train
        self.episode_id = 1
        self.today = date.today().strftime("%Y-%m-%d")
        self.min_reward = np.inf
        self.max_reward = -np.inf
        self.best_episode = None
        self.worse_episode = None
        self.is_eval = is_eval

        if self.continuous_actions:
            self.action_space = spaces.Box(low=-1, high=1, shape=(self.n_assets,), dtype=np.float32)
        else:
            self.action_set = self._valid_action_set()
            self.action_space = spaces.Discrete(len(self.action_set))

        self.feature_columns = [col for col in self.df.columns if col not in ["date", "ticker", "Open"]]
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.n_assets * (len(self.feature_columns) + 3),),
            dtype=np.float32
        )

        self.reset()

    def _valid_action_set(self):
        all_actions = list(product(range(7), repeat=self.n_assets))

        def compra_proportion(a):
            val = {0: 0.0, 1: 0.25, 2: 0.5, 3: 0.75, 4: 0.0, 5: 0.0, 6: 0.0}
            return val[a]

        valid_actions = [
            action for action in all_actions
            if sum(compra_proportion(a) for a in action) <= 1.0
        ]
        return valid_actions

    def reset(self, seed=None, options=None):
        self.max_steps = len(self.df) // self.n_assets - 1
        self.current_step = np.random.randint(1, self.max_steps - self.episode_length) if not self.is_eval else 0
        self.start_step = self.current_step
        self.balance = self.initial_balance
        self.shares_held = [0] * self.n_assets
        self.previous_net_worth = self.initial_balance
        self.steps_without_positions = 0
        self.asset_usage_counter = [0] * self.n_assets

        self.history = {
            "step": [self.start_step],
            "date": [self.df.iloc[self.start_step * self.n_assets]["date"]],
            "balance": [self.initial_balance],
            "net_worth": [self.initial_balance],
            "reward": [0],
            "action_vector": [[]],
            "shares_held_vector": [self.shares_held.copy()]  # üîÅ afegim aix√≤
        }

        return self._get_obs(), {}

    def _get_obs(self):
        obs = []
        for i in range(self.n_assets):
            row = self.df.iloc[self.current_step * self.n_assets + i]
            features = row[self.feature_columns].values
            obs.extend(np.concatenate([features, [self.balance], [self.previous_net_worth], [self.shares_held[i]]]))
        return np.array(obs, dtype=np.float32)

    def decode_action(self, action_idx):
        return list(self.action_set[action_idx])

    def step(self, action):
        if self.continuous_actions:
            # --- ACCIONS CONT√çNUES ---
            actions = np.clip(action, -1, 1)  # assegura accions dins [-1, 1]

            # Calcular proporcions de compra
            buy_props = np.clip(actions, 0, 1)
            total_buy_prop = np.sum(buy_props)
            if total_buy_prop > 1.0:
                buy_props = buy_props / total_buy_prop  # normalitza per evitar sobreassignaci√≥

            # Compres
            for i in range(self.n_assets):
                row = self.df.iloc[self.current_step * self.n_assets + i]
                price = row["Open"]
                prop = buy_props[i]
                if prop > 0:
                    cash = self.balance * prop
                    self._buy_fixed_cash(i, price, cash)

            # Vendes
            for i in range(self.n_assets):
                row = self.df.iloc[self.current_step * self.n_assets + i]
                price = row["Open"]
                sell_prop = -min(actions[i], 0.0)  # nom√©s si < 0
                if sell_prop > 0:
                    self._sell(i, price, sell_prop)

            actions_record = actions.tolist()

        else:
            # --- ACCIONS DISCRETES ---
            actions = self.decode_action(int(action))

            def get_buy_proportion(act):
                return {1: 0.25, 2: 0.5, 3: 0.75}.get(act, 0.0)

            # Calcula el cash a utilitzar per cada acci√≥ de compra
            cash_allocations = [self.balance * get_buy_proportion(act) for act in actions]

            for i in range(self.n_assets):
                row = self.df.iloc[self.current_step * self.n_assets + i]
                price = row["Open"]
                act = actions[i]

                if act in [1, 2, 3]:  # buy
                    self._buy_fixed_cash(i, price, cash_allocations[i])

                elif act in [4, 5, 6]:  # sell
                    proportion = {4: 0.25, 5: 0.5, 6: 1.0}[act]
                    self._sell(i, price, proportion)

            actions_record = actions

        # ‚úÖ Post-acci√≥
        self.current_step += 1
        net_worth = self.balance
        for i in range(self.n_assets):
            close_price = self.df.iloc[self.current_step * self.n_assets + i]["Close"]
            net_worth += self.shares_held[i] * close_price

        if sum(self.shares_held) == 0:
            self.steps_without_positions += 1
        else:
            self.steps_without_positions = 0

        for i, s in enumerate(self.shares_held):
            if s > 0:
                self.asset_usage_counter[i] += 1

        reward = self._compute_reward(net_worth)
        self.previous_net_worth = net_worth

        self.history["step"].append(self.current_step)
        self.history["date"].append(self.df.iloc[self.current_step * self.n_assets]["date"])
        self.history["balance"].append(self.balance)
        self.history["net_worth"].append(net_worth)
        self.history["reward"].append(reward)
        self.history["shares_held_vector"].append(self.shares_held.copy())
        self.history["action_vector"].append(actions_record)

        info = {}
        terminated = (
            self.current_step >= self.start_step + self.episode_length
            if not self.is_eval else
            self.current_step >= self.max_steps
        )
        truncated = net_worth <= self.initial_balance * 0.15
        if self.is_train and self.balance == self.initial_balance and self.current_step-self.start_step > 10:
            truncated = True
            reward  = -1000.0
            info["terminated_reason"] = "inactivity"

        if terminated or truncated:
            if self.episode_id % 10 == 0 and self.is_train:
                logger.info(
                    f"Episode {self.episode_id} finished at {self.current_step} after {self.current_step - self.start_step} steps")
            if self.do_save_history:
                if net_worth < self.min_reward:
                    self.min_reward = net_worth
                    self.worse_episode = self.episode_id
                if net_worth > self.max_reward:
                    self.max_reward = net_worth
                    self.best_episode = self.episode_id
            self.episode_id += 1

        return self._get_obs(), reward, terminated, truncated, info

    def _buy_fixed_cash(self, i, price, cash):
        if self.balance <= 0 or cash <= 0:
            return
        shares = int(cash // price)
        cost = shares * price
        if shares > 0 and cost <= self.balance:
            self.balance -= cost
            self.shares_held[i] += shares

    def _compute_reward(self, net_worth):
        # Premi base pel canvi absolut del patrimoni net
        reward = net_worth - self.previous_net_worth

        # Penalitzaci√≥ per ocasions perdudes
        missed_penalty = 0.0
        penalty_scale = 1000.0  if not self.continuous_actions else 10000 # Penalitzaci√≥ proporcional al canvi de preu si es perd una oportunitat clara
        threshold = 0.01  # Canvi del 1%

        if self.current_step > 1 and self.history["action_vector"]:
            try:
                last_actions = self.history["action_vector"][-1]

                for i in range(self.n_assets):
                    prev_price = self.df.iloc[(self.current_step - 1) * self.n_assets + i]["Close"]
                    curr_price = self.df.iloc[self.current_step * self.n_assets + i]["Close"]
                    price_change = (curr_price - prev_price) / (prev_price + 1e-8)

                    action = last_actions[i]

                    if self.continuous_actions:
                        # Accions cont√≠nues ‚Üí floats entre [-1, 1]
                        act_val = float(action)

                        # Penalitzaci√≥ per no comprar quan preu ha caigut ‚â• 1% i hi ha cash disponible
                        if price_change <= -threshold and self.balance > 0.25 * curr_price and act_val <= 0.05:
                            missed_penalty += abs(price_change) * penalty_scale

                        # Penalitzaci√≥ per no vendre quan el preu ha pujat ‚â• 1% i es tenen accions
                        if price_change >= threshold and self.shares_held[i] > 0 and act_val >= -0.05:
                            missed_penalty += abs(price_change) * penalty_scale

                    else:
                        # Accions discretes ‚Üí enters de 0 a 6
                        # [1,2,3] = compra | [4,5,6] = venda
                        if price_change <= -threshold and self.balance > 0.25 * curr_price and action not in [1, 2, 3]:
                            missed_penalty += abs(price_change) * penalty_scale

                        if price_change >= threshold and self.shares_held[i] > 0 and action not in [4, 5, 6]:
                            missed_penalty += abs(price_change) * penalty_scale

            except (IndexError, TypeError):
                pass  # Per seguretat, si no hi ha accions encara

        reward -= missed_penalty
        return reward

    def _sell(self, i, price, proportion):
        shares = int(self.shares_held[i] * proportion)
        if shares > 0:
            self.shares_held[i] -= shares
            self.balance += shares * price

    def render(self, mode="human"):
        logger.info(
            f"Step {self.current_step} | Balance: {self.balance:.2f} | Net Worth: {self.previous_net_worth:.2f} | Holdings: {self.shares_held}")

    def _compute_metrics(self, risk_free_rate: float = 0.05) -> dict:
        df = pd.DataFrame(self.history)
        net_worth = df["net_worth"].values
        rewards = df["reward"].values

        if len(net_worth) < 2:
            return {
                "Reward Sum": 0.0,
                "Sharpe Ratio": 0.0,
                "Max Drawdown": 0.0,
                "Mean Return": 0.0
            }

        # Recompensa acumulada (sumatori del reward)
        total_reward = rewards.sum()

        # Retorns percentuals diaris
        returns = np.diff(net_worth) / (net_worth[:-1] + 1e-8)

        # Retorn mitj√† diari
        mean_return = returns.mean()

        # Sharpe Ratio (N = 252 dies anuals)
        std_returns = returns.std()
        sharpe = (mean_return - risk_free_rate/252) / max(std_returns, 1e-6) * (252 ** 0.5)

        # Max Drawdown
        cum_max = np.maximum.accumulate(net_worth)
        drawdown = (cum_max - net_worth) / (cum_max + 1e-8)
        max_drawdown = drawdown.max()
        cumulative_return = net_worth[-1] / (net_worth[0] + 1e-8) - 1

        return {
            "Reward Sum": total_reward,
            "Sharpe Ratio": sharpe,
            "Max Drawdown": max_drawdown,
            "Mean Return": mean_return,
            "Cumulative Return": cumulative_return,        }

    def save_history(self, path: str):
        df_history = pd.DataFrame(self.history)
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        df_history.to_csv(path, index=False)
        logger.info(f"‚úÖ History saved to {path}")
        metrics = self._compute_metrics()
        metrics_path = path.with_suffix(".json")
        with open(metrics_path, "w") as f:
            json.dump(metrics, f, indent=2)
        logger.info(f"üìä Metrics saved to {metrics_path}")